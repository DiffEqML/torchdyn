{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trajectory Tracking with Integral Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdyn.models import *; from torchdyn.data_utils import *\n",
    "from torchdyn import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integral Loss\n",
    "\n",
    "We consider a loss of type\n",
    "\n",
    "$$\n",
    "    \\ell_\\theta := \\int_0^S g(h(s))d\\tau\n",
    "$$\n",
    "\n",
    "where $h(s)$ satisfies the neural ODE initial value problem \n",
    "\n",
    "$$\n",
    "    \\left\\{\n",
    "    \\begin{aligned}\n",
    "        \\dot{h}(s) &= f(h(s), \\theta(s))\\\\\n",
    "        h(0) &= x\n",
    "    \\end{aligned}\n",
    "    \\right. \\quad s\\in[0,S]\n",
    "$$\n",
    "\n",
    "where $\\theta(s)$ is parametrized with some spectral method (`Galerkin-style`),  i.e. $\\theta(s)=\\theta(s,\\omega)$ [$\\omega$: parameters of $\\theta(s)$].\n",
    "\n",
    "**REMARK:** In `torchdyn`, we do not need to evaluate the following integral in the formard pass of the ODE integration.\n",
    "In fact, we will compute the gradient $d\\ell/d\\omega$ just by solving backward \n",
    "\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "        \\dot\\lambda (s) &= -\\frac{\\partial f}{\\partial h}\\lambda(s) + \\frac{\\partial g}{\\partial h} \\\\\n",
    "        \\dot\\mu (s) &= -\\frac{\\partial f}{\\partial \\theta}\\frac{\\partial \\theta}{\\partial \\omega}\\lambda(s)\n",
    "    \\end{aligned}~~\\text{with}~~\n",
    "    \\begin{aligned}\n",
    "        \\lambda (S) &= 0\\\\\n",
    "        \\mu (S) &= 0\n",
    "    \\end{aligned}\n",
    "$$\n",
    "\n",
    "and, $\\frac{d\\ell}{d\\omega} = \\mu(0)$. Check out [this paper](https://arxiv.org/abs/2003.08063) for more details on the integral adjoint for neural ODEs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** Use a neural ODE to track a 2D curve $\\gamma:[0,\\infty]\\rightarrow \\mathbb{S}_2$ ($\\mathbb{S}_2$: unitary circle, $\\mathbb{S}_2:=\\{x\\in\\mathbb{R}^2:||x||_2=1\\}$), i.e.\n",
    "\n",
    "$$\n",
    "    \\gamma(s) := [\\cos(2\\pi s), \\sin(2\\pi s)]\n",
    "$$\n",
    "\n",
    "which has periodicity equal to $1$: $\\forall n\\in\\mathbb{N}, \\forall s\\in[0,\\infty]~~\\gamma(s) = \\gamma(ns)$.\n",
    "\n",
    "Let suppose to train the neural ODE for $s\\in[0,1]$. Therefore we can easily setup the integral cost as\n",
    "\n",
    "$$\n",
    "    \\ell_\\theta := \\int_0^1 ||h(\\tau)-\\gamma(\\tau)||_2^2 d\\tau\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cost(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.g = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, s, x):\n",
    "        self.y = torch.tensor([torch.cos(2*np.pi*s), torch.sin(2*np.pi*s)]).to(device)\n",
    "        self.y = self.y.repeat(x.shape[0]).reshape(-1,2)\n",
    "        return self.g(x, self.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** In this case we do not define any dataset of initial conditions and load it into the dataloader. Instead, at each step, we will sample new ICs from a normal distrubution centered in $\\gamma(0) = [1,0]$\n",
    "\n",
    "$$\n",
    "    x_t \\sim \\mathcal{N}(\\gamma(0),0.1)\n",
    "$$\n",
    "\n",
    "However, we still need to define a \"dummy\" `trainloader` to \"trick\" `pythorch_lightning`'s API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy trainloader\n",
    "train = torch.utils.data.TensorDataset(torch.zeros(1), torch.zeros(1))\n",
    "trainloader = torch.utils.data.DataLoader(train, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learner**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Learner(pl.LightningModule):\n",
    "    def __init__(self, model:nn.Module, settings:dict={}):\n",
    "        super().__init__()\n",
    "        defaults.update(settings)\n",
    "        self.settings = defaults\n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)    \n",
    "    \n",
    "    def training_step(self, batch, batch_idx):    \n",
    "        # We sample from Normal distribution around \"nominal\" initial condition\n",
    "        x = torch.tensor([1.,0.])\n",
    "        x = x + 0.1*torch.randn(100,2)\n",
    "        y_hat = self.model(x.to(device))\n",
    "        # We need to evaluate it so that we can \"trigger\" the backward pass through the adjoint \n",
    "        loss = y_hat.sum()\n",
    "        tensorboard_logs = {'train_loss': loss}\n",
    "        # At traing we expect `loss` to be 1\n",
    "        return {'loss': loss, 'log': tensorboard_logs}   \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.RMSprop(self.model.parameters(), lr=0.001)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return trainloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Varying Neural ODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In settings we have to specify both 'integral_adjoint' as \n",
    "# `backprop_style` and the 'cost' we previously defined.\n",
    "settings = {'type':'classic', 'backprop_style':'integral_adjoint', 'cost':Cost(),\n",
    "            'solver':'dopri5',  'atol':1e-6, 'rtol':1e-4}\n",
    "\n",
    "# We use a Galerkin neural ODE with one hidden layer, \n",
    "# Fourier spectrum (period=1) and only two freq.s\n",
    "f = DEFunc(nn.Sequential(DepthCat(1),\n",
    "                            GalLinear(2,64,FourierExpansion,2,2),\n",
    "                            nn.Tanh(),\n",
    "                            nn.Linear(64,2)                        \n",
    "                            ))\n",
    "\n",
    "# Define the model\n",
    "model = NeuralDE(f, settings).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightning:\n",
      "  | Name             | Type       | Params\n",
      "--------------------------------------------\n",
      "0 | model            | NeuralDE   | 898   \n",
      "1 | model.defunc     | DEFunc     | 898   \n",
      "2 | model.defunc.m   | Sequential | 898   \n",
      "3 | model.defunc.m.0 | DepthCat   | 0     \n",
      "4 | model.defunc.m.1 | GalLinear  | 768   \n",
      "5 | model.defunc.m.2 | Tanh       | 0     \n",
      "6 | model.defunc.m.3 | Linear     | 130   \n",
      "7 | model.adjoint    | Adjoint    | 0     \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef9f00134a074f649ce7e1687e3b3475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max=1.0), HTML(value='')), â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stefano\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\warnings.py:18: RuntimeWarning: Displayed epoch numbers in the progress bar start from \"1\" until v0.6.x, but will start from \"0\" in v0.8.0.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "learn = Learner(model)\n",
    "trainer = pl.Trainer(min_nb_epochs=1000, max_nb_epochs=1500)\n",
    "trainer.fit(learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plots**\n",
    "\n",
    "We first evaluate the model on a test set of initial conditions sampled from $\\mathcal{N}(\\gamma(0),\\sigma\\mathbb{I})$. Since we trained the neural ODE with $\\sigma=0.1$, now we test it with $\\sigma=0.2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_span = torch.linspace(0, 5, 500)\n",
    "x_test = torch.tensor([1.,0.])\n",
    "x_test = x_test + 0.2*torch.randn(100, 2)\n",
    "trajectory = model.trajectory(x_test.to(device), s_span).detach().cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Depth evolution of the system** -> The system is trained for $s\\in[0,1]$ and then we extrapolate until $s=5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = np.cos(2*np.pi*s_span)\n",
    "y2 = np.sin(2*np.pi*s_span)\n",
    "\n",
    "fig = plt.figure(figsize=(8,3))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(s_span[:100], y1[:100], color='orange',alpha=0.5)\n",
    "ax.scatter(s_span[:100], y2[:100], color='orange',alpha=0.5)\n",
    "ax.scatter(s_span[100:], y1[100:], color='red',alpha=0.5)\n",
    "ax.scatter(s_span[100:], y2[100:], color='red',alpha=0.5)\n",
    "for i in range(len(x_test)):\n",
    "    ax.plot(s_span, trajectory[:,i,:], color='blue', alpha=.1)\n",
    "ax.set_xlabel(r\"$s$ [depth]\")\n",
    "ax.set_ylabel(r\"$h(s)$ [state]\")\n",
    "ax.set_title(r\"Depth evolution of the system\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**State-space trajectories**\n",
    "-> All the (random) IC converge to the desired trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(3,3))\n",
    "ax = fig.add_subplot(111)\n",
    "for i in range(len(x_test)):\n",
    "    ax.plot(trajectory[:,i,0], trajectory[:,i,1], color='blue', alpha=.1)\n",
    "    ax.scatter(trajectory[0,i,0], trajectory[0,i,1], color='black', alpha=.1)\n",
    "ax.plot(y1[:100],y2[:100], color='red')\n",
    "ax.set_xlabel(r\"$h_0$\")\n",
    "ax.set_ylabel(r\"$h_1$\")\n",
    "ax.set_title(r\"State-Space Trajectories\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
