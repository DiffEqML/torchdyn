{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.metrics.functional import accuracy\n",
    "import torchdiffeq\n",
    "\n",
    "from torchdyn.models import *; from torchdyn import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we explore standard image classification on MNIST and CIFAR10 with convolutional neural ODE variants.\n",
    "* Depth-invariant neural ODE\n",
    "* Galerkin neural ODE (GalNODE)\n",
    "* Galerkin neural ODE with adjoint loss\n",
    "\n",
    "In the following notebooks we'll explore `augmentation` strategies that can be easily applied to the models below with the flexible `torchdyn` API. Here, we use simple `0-augmentation` (the ANODE model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "size=28\n",
    "path_to_data='../data/mnist_data'\n",
    "\n",
    "all_transforms = transforms.Compose([\n",
    "    transforms.Resize(size),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_data = datasets.MNIST(path_to_data, train=True, download=True,\n",
    "                            transform=all_transforms)\n",
    "test_data = datasets.MNIST(path_to_data, train=False,\n",
    "                           transform=all_transforms)\n",
    "\n",
    "trainloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "testloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Learner** is then defined as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner(pl.LightningModule):\n",
    "    def __init__(self, model:nn.Module):\n",
    "        super().__init__()\n",
    "        self.lr = 1e-3\n",
    "        self.model = model\n",
    "        self.iters = 0.\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        self.iters += 1.\n",
    "        x, y = batch   \n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_hat = self.model(x)   \n",
    "        loss = nn.CrossEntropyLoss()(y_hat, y)\n",
    "        \n",
    "        epoch_progress = self.iters / self.loader_len\n",
    "        acc = accuracy(y_hat, y)\n",
    "        nfe = model[2].nfe ; model[2].nfe = 0\n",
    "        tqdm_dict = {'train_loss': loss, 'accuracy': acc, 'NFE': nfe}\n",
    "        logs = {'train_loss': loss, 'epoch': epoch_progress}\n",
    "        return {'loss': loss, 'progress_bar': tqdm_dict, 'log': logs}   \n",
    "\n",
    "    def test_step(self, batch, batch_nb):\n",
    "        x, y = batch\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_hat = self(x)\n",
    "        acc = accuracy(y_hat, y)\n",
    "        return {'test_loss': nn.CrossEntropyLoss()(y_hat, y), 'test_accuracy': acc}\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        avg_acc = torch.stack([x['test_accuracy'] for x in outputs]).mean()\n",
    "        logs = {'test_loss': avg_loss}\n",
    "        return {'avg_test_loss': avg_loss, 'avg_test_accuracy': avg_acc,\n",
    "                'log': logs, 'progress_bar': logs}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=1e-4)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        self.loader_len = len(trainloader)\n",
    "        return trainloader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        self.test_loader_len = len(trainloader)\n",
    "        return testloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth-Invariant Neural ODE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = nn.Sequential(nn.Conv2d(6, 6, 3, padding=1),\n",
    "                    nn.Softplus(),\n",
    "                    nn.Conv2d(6, 6, 3, padding=1),\n",
    "                    nn.Softplus(),\n",
    "                    nn.Conv2d(6, 6, 3, padding=1) \n",
    "                    ).to(device)\n",
    "\n",
    "neuralDE = NeuralDE(func, \n",
    "                   solver='rk4',\n",
    "                   sensitivity='adjoint',\n",
    "                   s_span=torch.linspace(0, 1, 30)).to(device)\n",
    "\n",
    "model = nn.Sequential(Augmenter(augment_dims=5),\n",
    "                      nn.BatchNorm2d(6),\n",
    "                      neuralDE,\n",
    "                      nn.Conv2d(6, 1, 3, padding=1),\n",
    "                      nn.Flatten(),\n",
    "                      nn.Dropout(p=0.8),\n",
    "                      nn.Linear(28*28, 10)).to(device)\n",
    "\n",
    "\n",
    "logger = WandbLogger() # feel free to comment out or use a different logging scheme :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/zymrael/torchdyn-tutorials\" target=\"_blank\">https://app.wandb.ai/zymrael/torchdyn-tutorials</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/zymrael/torchdyn-tutorials/runs/23o1l9kr\" target=\"_blank\">https://app.wandb.ai/zymrael/torchdyn-tutorials/runs/23o1l9kr</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | Sequential | 8 K   \n",
      "/home/jyp/michael_dev/testenv/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:25: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2782d7f94cc044a2bc21eda3e69cab8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn = Learner(model)\n",
    "trainer = pl.Trainer(max_epochs=20,\n",
    "                     logger=logger,\n",
    "                     benchmark=True,\n",
    "                     limit_test_batches=0.25,\n",
    "                     gpus=1,\n",
    "                     progress_bar_refresh_rate=1\n",
    "                     )\n",
    "\n",
    "trainer.fit(learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 epochs are not enough. Feel free to keep training and using all kinds of scheduling and optimization tricks :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jyp/michael_dev/testenv/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:25: UserWarning: Your test_dataloader has shuffle=True, it is best practice to turn this off for validation and test dataloaders.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/jyp/michael_dev/testenv/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:25: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59978f2abb56446ab65bcd8295a37cd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "TEST RESULTS\n",
      "{'avg_test_accuracy': tensor(0.9804, device='cuda:0'),\n",
      " 'avg_test_loss': tensor(0.0550, device='cuda:0'),\n",
      " 'test_loss': tensor(0.0550, device='cuda:0')}\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.test(learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST (GalNODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {'type':'classic', 'controlled':False, 'solver':'dopri5', 'return_traj':False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = DEFunc(nn.Sequential(DepthCat(1),\n",
    "                            GalConv2d(6, 6, 3, padding=1, expfunc=FourierExpansion, n_harmonics=4, n_eig=1),\n",
    "                            nn.Tanh(),\n",
    "                            nn.Conv2d(6, 6, 3, padding=1),\n",
    "                            nn.Tanh(),\n",
    "                            nn.Conv2d(6, 6, 3, padding=1) \n",
    "                           )                       \n",
    "             ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuralDE = NeuralDE(func, settings).to(device)\n",
    "\n",
    "model = nn.Sequential(Augmenter(augment_dims=5),\n",
    "                      nn.BatchNorm2d(6),\n",
    "                      neuralDE,\n",
    "                      nn.Conv2d(6, 1, 3, padding=1),\n",
    "                      nn.Flatten(),\n",
    "                      nn.Linear(28*28, 10)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = WandbLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightning:\n",
      "   | Name               | Type        | Params\n",
      "-----------------------------------------------\n",
      "0  | model              | Sequential  | 11 K  \n",
      "1  | model.0            | Augmenter   | 0     \n",
      "2  | model.1            | BatchNorm2d | 12    \n",
      "3  | model.2            | NeuralDE    | 3 K   \n",
      "4  | model.2.defunc     | DEFunc      | 3 K   \n",
      "5  | model.2.defunc.m   | Sequential  | 3 K   \n",
      "6  | model.2.defunc.m.0 | DepthCat    | 0     \n",
      "7  | model.2.defunc.m.1 | GalConv2d   | 2 K   \n",
      "8  | model.2.defunc.m.2 | Tanh        | 0     \n",
      "9  | model.2.defunc.m.3 | Conv2d      | 330   \n",
      "10 | model.2.defunc.m.4 | Tanh        | 0     \n",
      "11 | model.2.defunc.m.5 | Conv2d      | 330   \n",
      "12 | model.2.adjoint    | Adjoint     | 0     \n",
      "13 | model.3            | Conv2d      | 55    \n",
      "14 | model.4            | Flatten     | 0     \n",
      "15 | model.5            | Linear      | 7 K   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbf249a281fa4520ba59285a68312aec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max=1.0), HTML(value='')), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn = Learner(model, lr=1e-3)\n",
    "trainer = pl.Trainer(min_nb_epochs=1, max_nb_epochs=2)\n",
    "trainer.fit(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jyp/.local/share/virtualenvs/GNODE--9w4TJnR/lib/python3.7/site-packages/pytorch_lightning/utilities/warnings.py:18: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a9ca16c17614c128174ee6968ca2d88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Testing', layout=Layout(flex='2'), max=157.0, style=Progr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "TEST RESULTS\n",
      "{'avg_test_accuracy': 91.18232727050781,\n",
      " 'avg_test_loss': 0.3063780665397644,\n",
      " 'test_loss': 0.3063780665397644}\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
