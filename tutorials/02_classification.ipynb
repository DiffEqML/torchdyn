{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import torchdiffeq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdyn.models import *; from torchdyn import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we explore standard image classification on MNIST and CIFAR10 with convolutional neural ODE variants.\n",
    "* Depth-invariant neural ODE\n",
    "* Galerkin neural ODE (GalNODE)\n",
    "\n",
    "In the following notebooks we'll explore `augmentation` strategies that can be easily applied to the models below with the flexible `torchdyn` API. Here, we use simple `0-augmentation` (the ANODE model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=256\n",
    "size=28\n",
    "path_to_data='../data/mnist_data'\n",
    "\n",
    "all_transforms = transforms.Compose([\n",
    "    transforms.Resize(size),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_data = datasets.MNIST(path_to_data, train=True, download=True,\n",
    "                            transform=all_transforms)\n",
    "test_data = datasets.MNIST(path_to_data, train=False,\n",
    "                           transform=all_transforms)\n",
    "\n",
    "trainloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "testloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Learner** is then defined as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner(pl.LightningModule):\n",
    "    def __init__(self, model:nn.Module):\n",
    "        super().__init__()\n",
    "        self.lr = 1e-3\n",
    "        self.model = model\n",
    "        self.iters = 0.\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        self.iters += 1.\n",
    "        x, y = batch   \n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_hat = self.model(x)   \n",
    "        loss = nn.CrossEntropyLoss()(y_hat, y)\n",
    "        print(f'\\r{self.lr}', end='')\n",
    "        epoch_progress = self.iters / self.loader_len\n",
    "        acc = accuracy(y_hat, y)\n",
    "        nfe = model[1].nfe ; model[1].nfe = 0\n",
    "        tqdm_dict = {'train_loss': loss, 'accuracy': acc, 'NFE': nfe}\n",
    "        logs = {'train_loss': loss, 'epoch': epoch_progress}\n",
    "        return {'loss': loss, 'progress_bar': tqdm_dict, 'log': logs}   \n",
    "\n",
    "    def test_step(self, batch, batch_nb):\n",
    "        x, y = batch\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_hat = self(x)\n",
    "        acc = accuracy(y_hat, y)\n",
    "        return {'test_loss': nn.CrossEntropyLoss()(y_hat, y), 'test_accuracy': acc}\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        avg_acc = torch.stack([x['test_accuracy'] for x in outputs]).mean()\n",
    "        logs = {'test_loss': avg_loss}\n",
    "        return {'avg_test_loss': avg_loss, 'avg_test_accuracy': avg_acc,\n",
    "                'log': logs, 'progress_bar': logs}\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.AdamW(self.parameters(), lr=self.lr, weight_decay=5e-5)\n",
    "        sched = {'scheduler': torch.optim.lr_scheduler.ReduceLROnPlateau(opt),\n",
    "                 'monitor': 'loss', \n",
    "                 'interval': 'step',\n",
    "                 'frequency': 10  }\n",
    "        return [opt], [sched]\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        self.loader_len = len(trainloader)\n",
    "        return trainloader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        self.test_loader_len = len(trainloader)\n",
    "        return testloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth-Invariant Conv Neural ODE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = nn.Sequential(nn.Conv2d(11, 12, 3, padding=1),\n",
    "                     nn.Softplus(),\n",
    "                     nn.Conv2d(12, 12, 5, padding=2),\n",
    "                     nn.Softplus(),\n",
    "                     nn.Conv2d(12, 11, 3, padding=1),                   \n",
    "                     ).to(device)\n",
    "\n",
    "neuralDE = NeuralDE(func, \n",
    "                   solver='rk4',\n",
    "                   sensitivity='autograd',\n",
    "                   s_span=torch.linspace(0, 1, 5)).to(device)\n",
    "\n",
    "model = nn.Sequential(Augmenter(augment_dims=10),\n",
    "                      neuralDE,\n",
    "                      nn.Conv2d(11, 1, 3, padding=1),\n",
    "                      nn.Flatten(),                     \n",
    "                      nn.Linear(28*28, 10)).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | Sequential | 13 K  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a9ce3bf65ef498389192b13be239459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), maxâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-b23b5960feba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m                      )\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, train_dataloader, val_dataloaders)\u001b[0m\n\u001b[0;32m    919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    920\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msingle_gpu\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 921\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msingle_gpu_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    922\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_tpu\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pragma: no-cover\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\distrib_parts.py\u001b[0m in \u001b[0;36msingle_gpu_train\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m    169\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreinit_scheduler_properties\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr_schedulers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_pretrain_routine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtpu_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtpu_core_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36mrun_pretrain_routine\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m   1089\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1090\u001b[0m         \u001b[1;31m# CORE TRAINING LOOP\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1091\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1092\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1093\u001b[0m     def test(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\training_loop.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    372\u001b[0m                 \u001b[1;31m# RUN TNG EPOCH\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m                 \u001b[1;31m# -----------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 374\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_training_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_steps\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_steps\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\training_loop.py\u001b[0m in \u001b[0;36mrun_training_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    455\u001b[0m             \u001b[1;31m# RUN TRAIN STEP\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m             \u001b[1;31m# ---------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m             \u001b[0m_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_training_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m             \u001b[0mbatch_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_norm_dic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_step_metrics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_outputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\training_loop.py\u001b[0m in \u001b[0;36mrun_training_batch\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m                 \u001b[1;31m# calculate loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 633\u001b[1;33m                 \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer_closure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    634\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    635\u001b[0m                 \u001b[1;31m# check if loss or model weights are nan\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\training_loop.py\u001b[0m in \u001b[0;36moptimizer_closure\u001b[1;34m()\u001b[0m\n\u001b[0;32m    595\u001b[0m                                                                     opt_idx, self.hiddens)\n\u001b[0;32m    596\u001b[0m                         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 597\u001b[1;33m                             \u001b[0moutput_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhiddens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    598\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m                         \u001b[1;31m# format and reduce outputs accordingly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\training_loop.py\u001b[0m in \u001b[0;36mtraining_forward\u001b[1;34m(self, batch, batch_idx, opt_idx, hiddens)\u001b[0m\n\u001b[0;32m    765\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransfer_batch_to_gpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgpu_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m             \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    768\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[1;31m# TPU support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-c8efc596c8d6>\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'\\r{self.lr}'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mepoch_progress\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miters\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloader_len\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mnfe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnfe\u001b[0m \u001b[1;33m;\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnfe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mtqdm_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'train_loss'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'NFE'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnfe\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'accuracy' is not defined"
     ]
    }
   ],
   "source": [
    "learn = Learner(model)\n",
    "trainer = pl.Trainer(max_epochs=10,\n",
    "                     gpus=1,\n",
    "                     progress_bar_refresh_rate=1,\n",
    "                     )\n",
    "\n",
    "trainer.fit(learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 epochs are not enough. Feel free to keep training and using all kinds of scheduling and optimization tricks :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e22511489d84bd3b092c83c048ef22a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "TEST RESULTS\n",
      "{'avg_test_accuracy': tensor(0.9187, device='cuda:0'),\n",
      " 'avg_test_loss': tensor(0.2766, device='cuda:0'),\n",
      " 'test_loss': tensor(0.2766, device='cuda:0')}\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.test(learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Galerkin Data-Controlled Conv Neural ODE (IL-Augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = nn.Sequential(DataControl(),\n",
    "                     DepthCat(1),\n",
    "                     GalConv2d(10+10, 12, 3, padding=1, expfunc=FourierExpansion, n_harmonics=5),\n",
    "                     nn.Softplus(),\n",
    "                     DataControl(),\n",
    "                     DepthCat(1),\n",
    "                     GalConv2d(22, 10, 3, padding=1, expfunc=FourierExpansion, n_harmonics=5),\n",
    "                     nn.Tanh()\n",
    "                     )\n",
    "\n",
    "neuralDE = NeuralDE(func, \n",
    "                   solver='rk4',\n",
    "                   sensitivity='autograd',\n",
    "                   s_span=torch.linspace(0, 1, 5)).to(device)\n",
    "\n",
    "model = nn.Sequential(Augmenter(augment_idx=1, augment_func=nn.Conv2d(1, 9, 3, padding=1)),\n",
    "                      neuralDE,\n",
    "                      nn.Conv2d(10, 1, 3, padding=1),\n",
    "                      nn.Flatten(),                     \n",
    "                      nn.Linear(28*28, 10)).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | Sequential | 49 K  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c61a16ada4741d29f675912f632f851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), maxâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn = Learner(model)\n",
    "trainer = pl.Trainer(max_epochs=10,\n",
    "                     gpus=1,\n",
    "                     progress_bar_refresh_rate=1,\n",
    "                     )\n",
    "\n",
    "trainer.fit(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jyp/michael_dev/testenv/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:25: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3461e88a3e534f02ae9009bdb7ce136b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "TEST RESULTS\n",
      "{'avg_test_accuracy': tensor(0.9580, device='cuda:0'),\n",
      " 'avg_test_loss': tensor(0.1408, device='cuda:0'),\n",
      " 'test_loss': tensor(0.1408, device='cuda:0')}\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.test(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
